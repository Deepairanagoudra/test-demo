{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd6a111-63b7-454c-a08d-96bf385c2b50",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipykernel_2169/1462103682.py:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m division\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mpyspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m sqlContext \u001b[38;5;241m=\u001b[39m pyspark\u001b[38;5;241m.\u001b[39mSQLContext(sc)\n\u001b[1;32m      6\u001b[0m sc\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql pyspark/2015-12-12.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m             use_unicode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39m\\\n\u001b[1;32m      8\u001b[0m take(\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipykernel_2169/1462103682.py:4 "
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "sqlContext = pyspark.SQLContext(sc)\n",
    "sc.textFile(\"sql pyspark/2015-12-12.csv\",\n",
    "            use_unicode=True).\\\n",
    "take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef294a19-b24c-4e10-8582-e246f0455bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date=datetime.date(2015, 12, 12), time=datetime.datetime(2023, 12, 28, 13, 42, 10), size=257886, r_version='3.2.2', r_arch='i386', r_os='mingw32', package='HistData', version='0.7-6', country='CZ', ip_id=1),\n",
       " Row(date=datetime.date(2015, 12, 12), time=datetime.datetime(2023, 12, 28, 13, 24, 37), size=1236751, r_version='3.2.2', r_arch='x86_64', r_os='mingw32', package='RJSONIO', version='1.3-0', country='DE', ip_id=2),\n",
       " Row(date=datetime.date(2015, 12, 12), time=datetime.datetime(2023, 12, 28, 13, 42, 35), size=2077876, r_version='3.2.2', r_arch='i386', r_os='mingw32', package='UsingR', version='2.0-5', country='CZ', ip_id=1),\n",
       " Row(date=datetime.date(2015, 12, 12), time=datetime.datetime(2023, 12, 28, 13, 42, 1), size=266724, r_version='3.2.2', r_arch='i386', r_os='mingw32', package='gridExtra', version='2.0.0', country='CZ', ip_id=1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CSVReader\").getOrCreate()\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = \"2015-12-12.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77624f2-d328-46ab-ab6e-153856eff5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37101d44-95b3-4332-8e8a-c07c516a0f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print('a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c9f65c8-868b-4fd7-bc0c-bd55dad2bafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session with a specific application name\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"MySparkApplication\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Check the Spark version\n",
    "print(\"Spark version: {}\".format(spark.version))\n",
    "\n",
    "# Your Spark-related code goes here\n",
    "\n",
    "# Stop the Spark session when you're done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31157f28-05b3-49bb-9f14-8f588df0729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f30f8534cd0>\n",
      "<function SparkSession.newSession at 0x7f30ab5874c0>\n",
      "<function SparkSession.Builder.getOrCreate at 0x7f30ab5871a0>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapplication').getOrCreate()\n",
    "spark2=SparkSession.newSession\n",
    "print(spark)\n",
    "print(spark2)\n",
    "spark3=SparkSession.Builder.getOrCreate\n",
    "print(spark3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e2714b8-a22c-4d31-a843-e861d1b78a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|  _1| _2|\n",
      "+----+---+\n",
      "| ram| 23|\n",
      "|sham| 24|\n",
      "| man| 25|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Myapplication').getOrCreate()\n",
    "df=spark.createDataFrame([('ram',23),('sham',24),('man',25)])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "246f0951-d434-4f0a-afae-9b125838b586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='default', catalog='spark_catalog', description='default database', locationUri='file:/home/jovyan/sql%2520pyspark/spark-warehouse')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapplication').getOrCreate()\n",
    "res=spark.catalog.listDatabases()\n",
    "print(res)\n",
    "tbls = spark.catalog.listTables()\n",
    "print(tbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ae99cd0-a387-4c10-9737-3e5e587ff658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=myapplication>\n",
      "myapplication\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapplication').getOrCreate()\n",
    "print(spark.sparkContext)\n",
    "print(spark.sparkContext.appName)\n",
    "spark.sparkContext.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f58a1962-3c8f-4be5-9c7e-617ce3e7a0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=MySparkApplication>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApplication\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Perform Spark operations using the SparkSession\n",
    "\n",
    "# Stop the SparkSession when the application is done\n",
    "spark.stop()\n",
    "context=spark.sparkContext\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6494b790-5b9d-4f5c-9d5a-792fe917a7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[5] at readRDDFromFile at PythonRDD.scala:289\n",
      "original data [1, 2, 3, 4, 5]\n",
      "after opertion [2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapplication').getOrCreate()\n",
    "context=spark.sparkContext\n",
    "data=[1,2,3,4,5]\n",
    "rdd=context.parallelize(data)\n",
    "print(rdd)\n",
    "res=rdd.map(lambda x:x*2)\n",
    "output=res.collect()\n",
    "print('original data',data)\n",
    "print('after opertion',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f15705a8-eecb-42e2-a0ac-0f7e8939825a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 1\n",
      "2: 1\n",
      "3: 1\n",
      "4: 1\n",
      "5: 1\n",
      "6: 1\n",
      "7: 1\n",
      "8: 1\n",
      "9: 3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"TextFileExample\").getOrCreate()\n",
    "\n",
    "# Specify the path to your text file\n",
    "file_path = \"test.txt\"\n",
    "\n",
    "# Create an RDD using SparkContext.textFile()\n",
    "rdd = spark.sparkContext.textFile(file_path)\n",
    "\n",
    "# Perform operations on the RDD\n",
    "word_count = rdd.flatMap(lambda line: line.split()).countByValue()\n",
    "\n",
    "# Display the results\n",
    "for word, count in word_count.items():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a54936c8-eed4-4400-8385-03dac9d3235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using whole text foile\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('file:/home/jovyan/sql pyspark/test.txt', '1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n9\\n9')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('myapplication').getOrCreate()\n",
    "# text_file_rdd=spark.sparkContext.textFile('test.txt')\n",
    "# print('using text file')\n",
    "# text_file_rdd.collect()\n",
    "\n",
    "text=spark.sparkContext.wholeTextFiles('test.txt')\n",
    "print('using whole text foile')\n",
    "text.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5e589f1b-eedc-4269-ac38-94276eb0531f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using textFile:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5', '6', '7', '8', '9', '9', '9']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "# Read the text file using textFile\n",
    "text_file_rdd = spark.sparkContext.textFile(\"test.txt\")\n",
    "\n",
    "# Show the content of the RDD\n",
    "print(\"Using textFile:\")\n",
    "text_file_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8563372f-7dfc-43db-8e95-bba5161ae640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using textFile:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('file:/home/jovyan/sql pyspark/test.txt', '1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n9\\n9')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "# Read the text file using textFile\n",
    "text_file_rdd = spark.sparkContext.wholeTextFiles(\"test.txt\")\n",
    "\n",
    "# Show the content of the RDD\n",
    "print(\"Using textFile:\")\n",
    "text_file_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9d64975e-cba9-4dc0-b9f2-c9a939fbd25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 'man']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapplication').getOrCreate()\n",
    "data=[1,2,3,4,5,'man']\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "af42dcd3-f954-4e84-886d-8f9dff89e8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapplication').getOrCreate()\n",
    "r=spark.sparkContext.emptyRDD()\n",
    "# r=spark.sparkContext.parallelize([])\n",
    "r.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6e8e28ad-acd6-44b0-8beb-8c02ff85993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "r=spark.sparkContext.parallelize([],4)\n",
    "print(r.collect())\n",
    "r.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f3c5e4ef-4503-47fd-b9ec-05f79ebbd9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|state     |\n",
      "+---------+--------+-------+----------+\n",
      "|James    |Smith   |USA    |California|\n",
      "|Michael  |Rose    |USA    |New York  |\n",
      "|Robert   |Williams|USA    |California|\n",
      "|Maria    |Jones   |USA    |Florida   |\n",
      "+---------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d434f8-d81c-4a9f-bc92-3ffc9d69b1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
